= Streaming ETL demo - Enriching event stream data with CDC data from MySQL, stream into Elasticsearch
Robin Moffatt <robin@confluent.io>
v1.31, May 10, 2019

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]

This is designed to be run as a step-by-step demo. The `ksql-statements.sql` should match those run in this doc end-to-end and in theory you can just run the file, but I have not tested it. PRs welcome for a one-click script that just demonstrates the end-to-end running demo :)

The slides that accompany this demo can be found here: https://speakerdeck.com/rmoff/apache-kafka-and-ksql-in-action-lets-build-a-streaming-data-pipeline

== Pre-reqs

Local:

* `curl`
* `jq`
* Docker

== Pre-Flight Setup

Start the environment

[source,bash]
----
docker-compose up -d
----

=== Run KSQL CLI and MySQL CLI

Optionally, use something like `screen` or `tmux` to have these both easily to hand. Or multiple Terminal tabs. Whatever works for you :)

* KSQL CLI:
+
[source,bash]
----
docker-compose exec ksql-cli bash -c 'echo -e "\n\n⏳ Waiting for KSQL to be available before launching CLI\n"; while [ $(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/) -eq 000 ] ; do echo -e $(date) "KSQL Server HTTP state: " $(curl -s -o /dev/null -w %{http_code} http://ksql-server:8088/) " (waiting for 200)" ; sleep 5 ; done; ksql http://ksql-server:8088'
----

* MySQL CLI:
+
[source,bash]
----
docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER -p$MYSQL_PASSWORD demo'
----

== Pre-flight checklist

* Load http://localhost:5601/app/kibana#/dashboard/mysql-ksql-kafka-es?_g=(refreshInterval:('$$hashKey':'object:229',display:'30%20seconds',pause:!f,section:1,value:30000),time:(from:now-15m,mode:quick,to:now))&_a=(description:'',filters:!(),fullScreenMode:!f,options:(darkTheme:!f,hidePanelTitles:!f,useMargins:!t),panels:!((gridData:(h:15,i:'1',w:24,x:0,y:10),id:'0c118530-31d5-11e8-a6be-09f3e3eb4b97',panelIndex:'1',type:visualization,version:'6.3.0'),(gridData:(h:10,i:'2',w:48,x:0,y:35),id:'39803a20-31d5-11e8-a6be-09f3e3eb4b97',panelIndex:'2',type:visualization,version:'6.3.0'),(gridData:(h:10,i:'4',w:8,x:0,y:0),id:'5ef922e0-6ff0-11e8-8fa0-279444e59a8f',panelIndex:'4',type:visualization,version:'6.3.0'),(gridData:(h:10,i:'5',w:40,x:8,y:0),id:'2f3d2290-6ff0-11e8-8fa0-279444e59a8f',panelIndex:'5',type:search,version:'6.3.0'),(gridData:(h:15,i:'6',w:24,x:24,y:10),id:c6344a70-6ff0-11e8-8fa0-279444e59a8f,panelIndex:'6',type:visualization,version:'6.3.0'),(embeddableConfig:(),gridData:(h:10,i:'7',w:48,x:0,y:25),id:'11a6f6b0-31d5-11e8-a6be-09f3e3eb4b97',panelIndex:'7',sort:!(EXTRACT_TS,desc),type:search,version:'6.3.0')),query:(language:lucene,query:''),timeRestore:!f,title:'Ratings%20Data',viewMode:view)[Kibana ratings dashboard]
* Create iTerm window, using the `screencapture` profile
* Load this instructions doc into Chrome
* Close all other apps
* Optional: 
** Make sure phone is on wifi/internet
** `cd ios_push_notifications`
** `python push_bullet.py`
** Connect laptop to wifi
** Connect iPhone to laptop with USB
** Launch QuickTime Player, select iPhone as source
** Disable other notifications (WhatsApp/Messages/Slack etc)

== Demo

image:images/ksql-debezium-es.png[Kafka Connect / KSQL / Elasticsearch]


== Part 01 - KSQL for filtering streams

=== Inspect topics

[source,sql]
----
SHOW TOPICS;
----

=== Inspect ratings & define stream

[source,sql]
----
CREATE STREAM RATINGS WITH (KAFKA_TOPIC='ratings',VALUE_FORMAT='AVRO');
----

=== Filter live stream of data

[source,sql]
----
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;
----

=== Create a derived stream

[source,sql]
----
CREATE STREAM POOR_RATINGS AS
SELECT STARS, CHANNEL, MESSAGE FROM RATINGS WHERE STARS<3;

SELECT * FROM POOR_RATINGS LIMIT 5;

DESCRIBE EXTENDED POOR_RATINGS;
----

Optionally, bring up a second KSQL prompt and show live ratings / live filtered ratings: 

[source,sql]
----
SET 'auto.offset.reset' = 'latest';
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') AS RATING_TIMESTAMP, STARS, CHANNEL FROM POOR_RATINGS;

SET 'auto.offset.reset' = 'latest';
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') AS RATING_TIMESTAMP, STARS, CHANNEL FROM RATINGS;
----

---

Return to slides 

---

== Part 02 - Kafka Connect

=== Kafka to Elasticsearch

See [kafka-to-es-demo.adoc](kafka-to-es-demo.adoc)

=== Show MySQL table + contents

[source,sql]
----
mysql> show tables;
+----------------+
| Tables_in_demo |
+----------------+
| CUSTOMERS      |
+----------------+
1 row in set (0.00 sec)

mysql> SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS LIMIT 5;
+----+-------------+------------+------------------------+-------------+
| ID | FIRST_NAME  | LAST_NAME  | EMAIL                  | CLUB_STATUS |
+----+-------------+------------+------------------------+-------------+
|  1 | Rica        | Blaisdell  | rblaisdell0@rambler.ru | bronze      |
|  2 | Ruthie      | Brockherst | rbrockherst1@ow.ly     | platinum    |
|  3 | Mariejeanne | Cocci      | mcocci2@techcrunch.com | bronze      |
|  4 | Hashim      | Rumke      | hrumke3@sohu.com       | platinum    |
|  5 | Hansiain    | Coda       | hcoda4@senate.gov      | platinum    |
+----+-------------+------------+------------------------+-------------+
5 rows in set (0.00 sec)
----

=== Check status of Debezium connectors

[source,bash]
----
curl -s "http://localhost:8083/connectors"| jq '.[]'| xargs -I{connector_name} curl -s "http://localhost:8083/connectors/"{connector_name}"/status"| jq -c -M '[.name,.connector.state,.tasks[].state]|join(":|:")'| column -s : -t| sed 's/\"//g'| sort
mysql-source-demo-CUSTOMERS      |  RUNNING  |  RUNNING
mysql-source-demo-CUSTOMERS-raw  |  RUNNING  |  RUNNING
----

=== Show Kafka topic has been created & populated

In KSQL: 

    LIST TOPICS;

    Kafka Topic                | Registered | Partitions | Partition Replicas | Consumers | ConsumerGroups
    --------------------------------------------------------------------------------------------------------
    _confluent-metrics         | false      | 12         | 1                  | 0         | 0
    _schemas                   | false      | 1          | 1                  | 0         | 0
    asgard                     | false      | 1          | 1                  | 0         | 0
    asgard-raw                 | false      | 1          | 1                  | 0         | 0
    asgard.demo.CUSTOMERS      | false      | 1          | 1                  | 0         | 0
    asgard.demo.CUSTOMERS-raw  | false      | 1          | 1                  | 0         | 0
    connect-status             | false      | 5          | 1                  | 0         | 0
    dbhistory.demo             | false      | 1          | 1                  | 0         | 0
    dbhistory.demo-raw         | false      | 1          | 1                  | 0         | 0
    docker-connect-configs     | false      | 1          | 1                  | 0         | 0
    docker-connect-offsets     | false      | 25         | 1                  | 0         | 0
    docker-connect-status      | false      | 5          | 1                  | 0         | 0
    jfokus19                     | false      | 1          | 1                  | 1         | 1
    my_connect_configs         | false      | 1          | 1                  | 0         | 0
    my_connect_offsets         | false      | 25         | 1                  | 0         | 0
    ratings                    | false      | 1          | 1                  | 0         | 0
    ratings-enriched           | false      | 1          | 1                  | 1         | 1
    UNHAPPY_PLATINUM_CUSTOMERS | false      | 1          | 1                  | 2         | 2
    --------------------------------------------------------------------------------------------------------

Show topic contents

    ksql> PRINT 'asgard.demo.CUSTOMERS' FROM BEGINNING;
    Format:AVRO
    11/13/18 12:52:09 PM UTC, , {"id": 1, "first_name": "Rica", "last_name": "Blaisdell", "email": "rblaisdell0@rambler.ru", "gender": "Female", "club_status": "bronze", "comments": "Universal optimal hierarchy", "create_ts": "2018-11-13T12:46:03Z", "update_ts": "2018-11-13T12:46:03Z", "messagetopic": "asgard.demo.CUSTOMERS", "messagesource": "Debezium CDC from MySQL on asgard"}
    11/13/18 12:52:09 PM UTC, , {"id": 2, "first_name": "Ruthie", "last_name": "Brockherst", "email": "rbrockherst1@ow.ly", "gender": "Female", "club_status": "platinum", "comments": "Reverse-engineered tangible interface", "create_ts": "2018-11-13T12:46:03Z", "update_ts": "2018-11-13T12:46:03Z", "messagetopic": "asgard.demo.CUSTOMERS", "messagesource": "Debezium CDC from MySQL on asgard"}

Create KSQL stream and table

[source,sql]
----
SET 'auto.offset.reset' = 'earliest';
CREATE STREAM CUSTOMERS_STREAM WITH (KAFKA_TOPIC='asgard.demo.CUSTOMERS', VALUE_FORMAT='AVRO');
CREATE STREAM CUSTOMERS_REKEYED WITH (PARTITIONS=1) AS SELECT * FROM CUSTOMERS_STREAM PARTITION BY ID;
-- This select statement is simply to make sure that we have time for the CUSTOMERS_REKEYED topic
-- to be created before we define a table against it
SELECT * FROM CUSTOMERS_REKEYED LIMIT 1;
CREATE TABLE CUSTOMERS WITH (KAFKA_TOPIC='CUSTOMERS_REKEYED',VALUE_FORMAT='AVRO',KEY='ID');
----

Query the KSQL table: 

[source,sql]
----
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS;
----


==== Insert a row in MySQL, observe it in Kafka

[source,sql]
----
INSERT INTO CUSTOMERS (ID,FIRST_NAME,LAST_NAME) VALUES (42,'Rick','Astley');
----

==== Update a row in MySQL, observe it in Kafka

[source,sql]
----
UPDATE CUSTOMERS SET EMAIL = 'rick@example.com' where ID=42;
UPDATE CUSTOMERS SET CLUB_STATUS = 'bronze' where ID=42;
UPDATE CUSTOMERS SET CLUB_STATUS = 'platinum' where ID=42;
----


Here's the table - the latest value for a given key
[source,sql]
----
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS WHERE ID=42;
----

[source,sql]
----
42 | Rick | Astley | rick@example.com | platinum
^CQuery terminated
----

==== [Optional] Demonstrate Stream / Table difference

Here's the stream - every event, which in this context is every change event on the source database: 

[source,sql]
----
SELECT ID, FIRST_NAME, LAST_NAME, EMAIL, CLUB_STATUS FROM CUSTOMERS_STREAM WHERE ID=42;
----

[source,sql]
----
42 | Rick | Astley | null | null
42 | Rick | Astley | rick@example.com | null
42 | Rick | Astley | rick@example.com | bronze
42 | Rick | Astley | rick@example.com | platinum
^CQuery terminated
ksql>
----

Optionally, point out before/after records in `raw` stream

[source,bash]
----
docker-compose exec -T kafka-connect \
        kafka-avro-console-consumer \
        --bootstrap-server kafka:29092 \
        --property schema.registry.url=http://schema-registry:8081 \
        --topic asgard.demo.CUSTOMERS-raw --from-beginning|jq -c '.'
----

---

Return to slides 

---

== Part 03 - KSQL for joining streams


=== Join live stream of ratings to customer data

[source,sql]
----
SELECT R.RATING_ID, R.MESSAGE, 
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, 
      C.CLUB_STATUS 
      FROM RATINGS R 
        LEFT JOIN CUSTOMERS C 
        ON R.USER_ID = C.ID 
      WHERE C.FIRST_NAME IS NOT NULL;
----

[source,sql]
----
524 | Surprisingly good, maybe you are getting your mojo back at long last! | Patti Rosten | silver
525 | meh | Fred Blaisdell | bronze
526 | more peanuts please | Hashim Rumke | platinum
527 | more peanuts please | Laney Toopin | platinum
529 | Exceeded all my expectations. Thank you ! | Ruthie Brockherst | platinum
530 | (expletive deleted) | Brianna Paradise | bronze
…
----

Persist this stream of data

[source,sql]
----
CREATE STREAM RATINGS_WITH_CUSTOMER_DATA 
       WITH (PARTITIONS=1, 
             KAFKA_TOPIC='ratings-enriched') 
       AS 
SELECT R.RATING_ID, R.MESSAGE, R.STARS, R.CHANNEL,
      C.ID, C.FIRST_NAME + ' ' + C.LAST_NAME AS FULL_NAME, 
      C.CLUB_STATUS, C.EMAIL 
      FROM RATINGS R 
        LEFT JOIN CUSTOMERS C 
        ON R.USER_ID = C.ID 
      WHERE C.FIRST_NAME IS NOT NULL;
----

The `WITH (PARTITIONS=1)` is only necessary if the Elasticsearch connector has already been defined, as it will create the topic before KSQL does, and using a single partition (not 4, as KSQL wants to by default).

=== Examine changing reference data

CUSTOMERS is a KSQL _table_, which means that we have the latest value for a given key.

Check out the ratings for customer id 2 only:
[source,sql]
----
ksql> SELECT FULL_NAME, CLUB_STATUS, STARS, MESSAGE, CHANNEL FROM RATINGS_WITH_CUSTOMER_DATA WHERE ID=2;
----

In mysql, make a change to ID 2
[source,sql]
----
mysql> UPDATE CUSTOMERS SET CLUB_STATUS = 'Platinum' WHERE ID=2;
----

Observe in the continuous KSQL query that the customer name has now changed.

=== Create stream of unhappy VIPs

[source,sql]
----
CREATE STREAM UNHAPPY_PLATINUM_CUSTOMERS 
       WITH (VALUE_FORMAT='JSON', PARTITIONS=1) AS 
SELECT FULL_NAME, CLUB_STATUS, EMAIL, STARS, MESSAGE 
FROM   RATINGS_WITH_CUSTOMER_DATA 
WHERE  STARS < 3 
  AND  CLUB_STATUS = 'platinum';
----

== View in Elasticsearch and Kibana

Tested on Elasticsearch 6.3.0

image:images/es01.png[Kibana]

---

Return to slides 

---

#EOF

== Optional


=== Aggregations

Simple aggregation - count of ratings per person, per minute:

[source,sql]
----
ksql> SELECT TIMESTAMPTOSTRING(WindowStart(), 'yyyy-MM-dd HH:mm:ss') AS WINDOW_START_TS, FULL_NAME,COUNT(*) FROM RATINGS_WITH_CUSTOMER_DATA WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
----

Persist this and show the timestamp:

[source,sql]
----
CREATE TABLE RATINGS_PER_CUSTOMER_PER_MINUTE AS SELECT FULL_NAME,COUNT(*) AS RATINGS_COUNT FROM ratings_with_customer_data WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY FULL_NAME;
SELECT TIMESTAMPTOSTRING(ROWTIME, 'yyyy-MM-dd HH:mm:ss') , FULL_NAME, RATINGS_COUNT FROM RATINGS_PER_CUSTOMER_PER_MINUTE;
----

=== Slack/PushBullet notifications

_This bit will need some config of your own, as you'll need your own Slack workspace and API key (both free). With this though, you can demo the idea of an event-driven app subscribing to a KSQL-populated stream of filtered events._

_A newer version of the push notification script uses PushBullet, see `ios_push_notifications/push_bullet.py`._

image:images/slack_ratings.png[Slack push notifications driven from Kafka and KSQL]

To run, first export your API key as an environment variable:

[source,bash]
----
export SLACK_API_TOKEN=xyxyxyxyxyxyxyxyxyxyxyx
----

Or if you've got it locally, run `source slack_creds.sh`

then run the code:

[source,bash]
----
python python_kafka_notify.py
----

You will need to install `slackclient` and `confluent_kafka` libraries.

